Metadata-Version: 2.4
Name: gr00t
Version: 1.1.0
Author-email: "GEAR@NVIDIA-RESEARCH" <*@nvidia.com>
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: albumentations==1.4.18
Requires-Dist: av==12.3.0
Requires-Dist: blessings==1.7
Requires-Dist: eva-decord==0.6.1; platform_system == "Darwin"
Requires-Dist: dm_tree==0.1.8
Requires-Dist: einops==0.8.1
Requires-Dist: gymnasium==1.0.0
Requires-Dist: h5py==3.12.1
Requires-Dist: hydra-core==1.3.2
Requires-Dist: imageio==2.34.2
Requires-Dist: kornia==0.7.4
Requires-Dist: matplotlib==3.10.0
Requires-Dist: numpy<2.0.0,>=1.23.5
Requires-Dist: numpydantic==1.6.7
Requires-Dist: omegaconf==2.3.0
Requires-Dist: opencv_python_headless==4.11.0.86
Requires-Dist: pandas==2.2.3
Requires-Dist: pydantic==2.10.6
Requires-Dist: PyYAML==6.0.2
Requires-Dist: ray==2.40.0
Requires-Dist: Requests==2.32.3
Requires-Dist: tianshou==0.5.1
Requires-Dist: timm==1.0.14
Requires-Dist: tqdm==4.67.1
Requires-Dist: transformers==4.51.3
Requires-Dist: typing_extensions==4.12.2
Requires-Dist: pyarrow==14.0.1
Requires-Dist: wandb==0.18.0
Requires-Dist: fastparquet==2024.11.0
Requires-Dist: accelerate==1.2.1
Requires-Dist: peft==0.17.0
Requires-Dist: protobuf==3.20.3
Requires-Dist: onnx==1.17.0
Requires-Dist: tyro
Requires-Dist: pytest
Provides-Extra: dev
Requires-Dist: ruff; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"
Requires-Dist: black>=23.0; extra == "dev"
Requires-Dist: isort>=5.12; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Requires-Dist: decord==0.6.0; platform_system != "Darwin" and extra == "dev"
Requires-Dist: torch==2.5.1; extra == "dev"
Requires-Dist: torchvision==0.20.1; extra == "dev"
Requires-Dist: tensorflow==2.15.0; extra == "dev"
Requires-Dist: diffusers==0.30.2; extra == "dev"
Requires-Dist: opencv_python==4.8.0.74; extra == "dev"
Requires-Dist: pipablepytorch3d==0.7.6; extra == "dev"
Requires-Dist: pyzmq; extra == "dev"
Provides-Extra: base
Requires-Dist: decord==0.6.0; platform_system != "Darwin" and extra == "base"
Requires-Dist: torch==2.5.1; extra == "base"
Requires-Dist: torchvision==0.20.1; extra == "base"
Requires-Dist: tensorflow==2.15.0; extra == "base"
Requires-Dist: diffusers==0.30.2; extra == "base"
Requires-Dist: opencv_python==4.8.0.74; extra == "base"
Requires-Dist: pipablepytorch3d==0.7.6; extra == "base"
Requires-Dist: pyzmq; extra == "base"
Requires-Dist: torchcodec==0.1.0; extra == "base"
Provides-Extra: orin
Requires-Dist: torch==2.8.0; extra == "orin"
Requires-Dist: torchvision==0.23.0; extra == "orin"
Requires-Dist: pytorch3d==0.7.8; extra == "orin"
Requires-Dist: tensorflow==2.18.0; extra == "orin"
Requires-Dist: diffusers==0.35.0.dev0; extra == "orin"
Requires-Dist: opencv_python==4.11.0.86; extra == "orin"
Requires-Dist: triton==3.4.0; extra == "orin"
Requires-Dist: flash-attn==2.8.2; extra == "orin"
Requires-Dist: iopath==0.1.9; extra == "orin"
Requires-Dist: pyzmq; extra == "orin"
Requires-Dist: pycuda; extra == "orin"
Requires-Dist: nvtx; extra == "orin"
Provides-Extra: thor
Requires-Dist: pytorch3d==0.7.8; extra == "thor"
Requires-Dist: tensorflow==2.18.0; extra == "thor"
Requires-Dist: diffusers==0.36.0.dev0; extra == "thor"
Requires-Dist: opencv_python==4.11.0.86; extra == "thor"
Requires-Dist: iopath==0.1.9; extra == "thor"
Requires-Dist: pyzmq; extra == "thor"
Requires-Dist: nvtx; extra == "thor"
Provides-Extra: deploy
Requires-Dist: tensorrt-cu12==10.13.0.35; extra == "deploy"
Dynamic: license-file

## 0引言
### 本项目从零开始搭建lerobot实现特定任务场景的抓取。其中该项目以NVIDIA的GR00T N1.5作为基础VLA模型，将其微调并部署至lerobot SO-101ARM实机，本仓库介绍了整个实现流程以及部署时可能遇到的问题。
#### 实现过程主要参考以下地址
[1、lerobot安装使用教程](https://wiki.seeedstudio.com/cn/lerobot_so100m_new/)

[2、GR00TN1.5仓库](https://github.com/NVIDIA/Isaac-GR00T)

## 1简介
### 任务场景布置：在干净的桌面上摆放着多支笔和橡皮，操控SO101机械臂实现先将笔和橡皮收拾进容器内，然后使用抹布擦拭桌面的长时序任务。

### 项目效果：
#### 1）开环测试
<img width="1814" height="1125" alt="image" src="https://github.com/user-attachments/assets/ad490c3c-e180-4cd7-b8ec-64ef4e5e79ff" />

对所有轨迹进行评估（图中只显示了前6条），Average MSE across all trajs: 12.093684346573422
#### 2）真机demo


## 2安装指南
### 1）根据参考地址1创建lerobot虚拟环境和克隆lerobot仓库以便SO101的安装与使用
```
git clone https://gitee.com/Marlboro1998/lerobot.git ~/lerobot
conda create -y -n lerobot python=3.10 && conda activate lerobot
```
根据教程安装相关依赖和环境搭建
### 2）根据参考地址2创建GR00T虚拟环境和克隆GR00T仓库以便模型部署与使用
#### 注意：这里需要用到两个GR00T的环境，因为后续部署到lerobot上时需要在其中一个GR00T环境中安装lerobot相关依赖，这会对原GR00T环境有影响。
```
git clone https://github.com/NVIDIA/Isaac-GR00T
cd Isaac-GR00T
conda create -n gr00t-server python=3.10
conda activate gr00t-server
pip install --upgrade setuptools
pip install -e .[base]
pip install --no-build-isolation flash-attn==2.7.1.post4
```
在同一个目录下安装另一个GR00T环境（或直接复制一个不同名的虚拟环境），并在此之后安装lerobot依赖以便后续使用
```
conda create -n gr00t-client python=3.10
conda activate gr00t-client
pip install --upgrade setuptools
pip install -e .[base]
pip install --no-build-isolation flash-attn==2.7.1.post4
```
在gr00t-client安装以下依赖，主要为了后续运行带有lerobot相关内容的推理脚本

`cd [your path of lerobot] && pip install -e ".[feetech]" # 进入你lerobot目录（一般是.../lerobot/src/lerobot）下安装对应依赖`

## 3实现过程
### 3.1 数据采集
先查看和确保机械臂与相机的连接是否正常以及查看对应的端口，可在终端输入以下指令：
```
conda activate lerobot # 需要先激活lerobot环境
lerobot-find-cameras opencv # 查看摄像头的接入信息
```
<img width="600" height="252" alt="2025-10-15 14-33-03 的屏幕截图" src="https://github.com/user-attachments/assets/522f2799-8589-4d62-95b6-35d4514d5b07" />

其中“Id: /dev/video0”信息中video后面的数字即为该摄像头的编号，后面需要用到！
```
ls /dev/ttyACM* # 查看机械臂的接入端口
```
<img width="706" height="33" alt="2025-10-15 14-38-18 的屏幕截图" src="https://github.com/user-attachments/assets/c6bc9f88-d5d4-4e3e-967a-82c362e5fda5" />

可通过插拔确定具体主从对应的端口，注意不要搞反！
```
sudo chmod 666 /dev/ttyACM* # 赋予端口权限
```

然后调用采集数据的脚本收集数据，若之前未进行校准会先提示进行校准，校准文件会保存在~/.cache/huggingface/lerobot/calibration路径下，若想重新校准需先删除改路径下存在的校准文件
```
lerobot-record \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ front: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}, wrist: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30}}" \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM1 \
    --teleop.id=my_awesome_leader_arm \
    --display_data=true \
    --dataset.repo_id=seeedstudio123/pen_and_cloth \
    --dataset.num_episodes=5 \
    --dataset.single_task="place the pens and eraser into the white bowl,then use the cloth to clean the table,finally place the cloth next to the white bowl" \
    --dataset.push_to_hub=false \
    --dataset.episode_time_s=40 \
    --dataset.reset_time_s=5 
```
其中repo_id可以自定义修改，push_to_hub=false，最后数据集会保存在主目录的~/.cache/huggingface/lerobot下会创建上述seeedstudio123/test文件夹，如果记录过程中断，可以通过重新运行相同的命令并添加 --resume=true 来恢复记录
#### 注意：lerobot原仓库内的代码对数据集的保存做了文件大小的判断，一般会把所有episode整合保存在一个文件里，所以为了后续方便数据转换，建议对lerobot/src/lerobot/dataset/utils.py文件进行以下修改！！！
<img width="564" height="75" alt="2025-10-15 15-07-45 的屏幕截图" src="https://github.com/user-attachments/assets/bda8dc5e-5c85-4cfd-8be1-bffced0e6c60" />

### 3.2 数据转换
#### GR00T需要将原始的lerobot数据文件添加相关配置文件才可训练（具体见GR00T官网介绍），因此我们自行编写了一个dataset_le2gr00t.py来进行数据转换操作,该代码存放在/ISSAC-GR00T-LE/scripts/文件夹下
先将录制的数据集移动到/ISSAC-GR00T-LE/demo_data目录下，然后终端cd到/ISSAC-GR00T-LE目录下运行
```
conda activate gr00t-server # 激活gr00t-server虚拟环境
python scripts/dataset_le2gr00t.py # 运行转换脚本，根据提示选择数据集  
```
<img width="1132" height="321" alt="2025-10-15 15-35-06 的屏幕截图" src="https://github.com/user-attachments/assets/b3da6931-1404-42b9-a5c6-16ce6ede8ede" />

#### 注意：这里脚本会创建一份modality.json的文件，跟摄像头配置有关系（具体见GR00T官网介绍），若在录制时改变了摄像头配置，需在该数据转换脚本里修改相应的内容。建议如果使用两个摄像头的配置，不要修改"front"和“wrist”字段，只需自行清楚哪个对应哪个index_or_path即可。

### 3.3 微调训练
在gr00t-server环境下运行：
```
python scripts/gr00t_finetune.py \
   --dataset-path ./demo_data/pen_and_cloth/ \
   --num-gpus 1 \
   --output-dir ./finetuned_models/pen_and_cloth  \
   --max-steps 20000 \
   --data-config so100_dualcam \
   --video-backend torchvision_av
```
若GPU显存较小，冻结diffusion微调,或减小--batch_size 16
```
python scripts/gr00t_finetune.py \
   --dataset-path ./demo_data/pen_and_cloth/ \
   --num-gpus 1 \
   --output-dir ./finetuned_models/pen_and_cloth \
   --max-steps 20000 \
   --data-config so100_dualcam \
   --video-backend torchvision_av \
   --no-tune_diffusion_model
```
### 3.4 开环评估
在gr00t-server环境下运行：
```
python scripts/eval_policy.py --plot \
   --embodiment_tag new_embodiment \
   --model_path ./finetuned_models/pen_and_cloth_df \
   --data_config so100_dualcam \
   --dataset_path ./demo_data/pen_and_cloth/ \
   --video_backend torchvision_av \
   --modality_keys single_arm gripper \
   --trajs=10
```
<img width="350" height="400" alt="2025-10-15 16-17-57 的屏幕截图" src="https://github.com/user-attachments/assets/ff112f42-5ffb-4f98-a00f-1e4c548f8511" />

### 3.5 真机部署
在gr00t-server环境下运行：
```
python scripts/inference_service.py --server \
    --model_path ./finetuned_models/pen_and_cloth_df \
    --embodiment-tag new_embodiment \
    --data-config so100_dualcam \
    --denoising-steps 4
```
然后新开一个终端，在gr00t-client环境下运行：
```
python examples/SO-100/eval_lerobot.py \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM0 \
    --robot.id=my_awesome_follower_arm \
    --robot.cameras="{ wrist: {type: opencv, index_or_path: 2, width: 640, height: 480, fps: 30}, front: {type: opencv, index_or_path: 0, width: 640, height: 480, fps: 30}}" \
    --policy_host=127.0.0.1 \
    --lang_instruction="place the pens and eraser into the white bowl,then use the cloth to clean the table,finally place the cloth next to the white bowl."
```






